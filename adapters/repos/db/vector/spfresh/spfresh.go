//                           _       _
// __      _____  __ ___   ___  __ _| |_ ___
// \ \ /\ / / _ \/ _` \ \ / / |/ _` | __/ _ \
//  \ V  V /  __/ (_| |\ V /| | (_| | ||  __/
//   \_/\_/ \___|\__,_| \_/ |_|\__,_|\__\___|
//
//  Copyright Â© 2016 - 2025 Weaviate B.V. All rights reserved.
//
//  CONTACT: hello@weaviate.io
//

package spfresh

import (
	"context"
	"sync"

	"github.com/pkg/errors"
	"github.com/puzpuzpuz/xsync/v4"
	"github.com/sirupsen/logrus"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/common"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/compressionhelpers"
	enterrors "github.com/weaviate/weaviate/entities/errors"
)

const (
	reassignThreshold = 3        // Fine-tuned threshold to avoid unnecessary splits during reassign operations
	splitReuseEpsilon = 0.000001 // Epsilon to determine if a split can reuse the existing posting
)

var (
	ErrPostingNotFound  = errors.New("posting not found")
	ErrVectorNotFound   = errors.New("vector not found")
	ErrIdenticalVectors = errors.New("posting list contains identical or near-identical vectors")
)

type SplitOperation struct {
	PostingID uint64
}

type MergeOperation struct {
	PostingID uint64
}

type ReassignOperation struct {
	PostingID uint64
	Vector    Vector
}

// SPFresh manages background operations for postings in the SPFresh index.
// It handles split, merge, and reassign requests generated by insertions or during search
// operations. It uses a worker pool to process these operations concurrently.
type SPFresh struct {
	Logger       *logrus.Entry
	UserConfig   *UserConfig                      // UserConfig contains user-defined settings for the rebuilder.
	SPTAG        SPTAG                            // SPTAG provides access to the SPTAG index for centroid operations.
	Store        *LSMStore                        // Used for managing persistence of postings.
	Splitter     PostingSplitter                  // Used for splitting postings into two.
	VersionMap   *VersionMap                      // Provides access to vector versions.
	IDs          *common.MonotonicCounter[uint64] // Shared monotonic counter for generating unique IDs for new postings.
	PostingSizes *PostingSizes                    // Stores the size of each posting
	Quantizer    *compressionhelpers.RotationalQuantizer

	// ctx and cancel are used to manage the lifecycle of the LocalRebuilder.
	ctx    context.Context
	cancel context.CancelFunc

	splitCh    chan SplitOperation    // Channel for split operations
	mergeCh    chan MergeOperation    // Channel for merge operations
	reassignCh chan ReassignOperation // Channel for reassign operations
	wg         sync.WaitGroup

	splitList *deduplicator // Prevents duplicate split operations
	mergeList *deduplicator // Prevents duplicate merge operations

	postingLocks *common.HashedLocks // Locks to prevent concurrent modifications to the same posting.
}

func (s *SPFresh) Start(ctx context.Context) {
	if s.UserConfig == nil {
		panic("UserConfig must be set before starting LocalRebuilder")
	}
	if s.Store == nil {
		panic("Store must be set before starting LocalRebuilder")
	}
	if s.Splitter == nil {
		panic("Splitter must be set before starting LocalRebuilder")
	}
	if s.VersionMap == nil {
		panic("VersionMap must be set before starting LocalRebuilder")
	}
	if s.IDs == nil {
		panic("IdGenerator must be set before starting LocalRebuilder")
	}
	if s.Quantizer == nil {
		panic("Quantizer must be set before starting LocalRebuilder")
	}

	s.ctx, s.cancel = context.WithCancel(ctx)

	if s.Logger == nil {
		s.Logger = logrus.New().WithField("component", "LocalRebuilder")
	} else {
		s.Logger = s.Logger.WithField("component", "LocalRebuilder")
	}

	s.postingLocks = common.NewHashedLocks32k()
	s.splitCh = make(chan SplitOperation, s.UserConfig.SplitWorkers*100)          // TODO: fine-tune buffer size
	s.mergeCh = make(chan MergeOperation, 1024)                                   // TODO: fine-tune buffer size
	s.reassignCh = make(chan ReassignOperation, s.UserConfig.ReassignWorkers*100) // TODO: fine-tune buffer size
	s.splitList = newDeduplicator()
	s.mergeList = newDeduplicator()

	// start N workers to process split operations
	for i := 0; i < s.UserConfig.SplitWorkers; i++ {
		s.wg.Add(1)
		enterrors.GoWrapper(s.splitWorker, s.Logger)
	}

	// start M workers to process reassign operations
	for i := 0; i < s.UserConfig.ReassignWorkers; i++ {
		s.wg.Add(1)
		enterrors.GoWrapper(s.reassignWorker, s.Logger)
	}

	// start a single worker to process merge operations
	s.wg.Add(1)
	enterrors.GoWrapper(s.mergeWorker, s.Logger)
}

// Insert a vector into one or more postings.
// The number of replicas is determined by the UserConfig.Replicas setting.
func (s *SPFresh) Insert(id uint64, vector []byte) error {
	replicas, _, err := s.selectReplicas(vector, 0)
	if err != nil {
		return err
	}

	s.VersionMap.AllocPageFor(id)

	for _, replica := range replicas {
		_, err = s.Append(Vector{ID: id, Data: vector, Version: s.VersionMap.Get(id)}, replica, false)
		if err != nil {
			return errors.Wrapf(err, "failed to append vector %d to replica %d", id, replica)
		}
	}

	return nil
}

func (s *SPFresh) Append(vector Vector, postingID uint64, reassigned bool) (bool, error) {
	s.postingLocks.Lock(postingID)

	// check if the posting still exists
	if !s.SPTAG.Exists(postingID) {
		// the vector might have been deleted concurrently,
		// might happen if we are reassigning
		if s.VersionMap.Get(vector.ID) == vector.Version {
			err := s.enqueueReassign(s.ctx, postingID, vector)
			s.postingLocks.Unlock(postingID)
			if err != nil {
				return false, err
			}
		}

		return false, nil
	}

	// append the new vector to the existing posting
	err := s.Store.Merge(s.ctx, postingID, vector)
	if err != nil {
		s.postingLocks.Unlock(postingID)
		return false, err
	}
	// increment the size of the posting
	s.PostingSizes.Inc(postingID, 1)

	s.postingLocks.Unlock(postingID)

	// ensure the posting size is within the configured limits
	count := s.PostingSizes.Get(postingID)

	// If the posting is too big, we need to split it.
	// During an insert, we want to split asynchronously
	// however during a reassign, we want to split immediately.
	// Also, reassign operations may cause the posting to grow beyond the max size
	// temporarily. To avoid triggering unnecessary splits, we add a fine-tuned threshold.
	max := s.UserConfig.MaxPostingSize
	if reassigned {
		max += reassignThreshold
	}
	if count > max {
		if reassigned {
			err = s.doSplit(postingID)
		} else {
			err = s.EnqueueSplit(s.ctx, postingID)
		}
		if err != nil {
			return false, err
		}
	}

	return true, nil
}

// Delete marks a vector as deleted in the version map.
func (s *SPFresh) Delete(ctx context.Context, id uint64) error {
	version := s.VersionMap.MarkDeleted(id)
	if version == 0 {
		return ErrVectorNotFound
	}

	return nil
}

func (s *SPFresh) Close(ctx context.Context) error {
	if s.ctx == nil {
		return nil // Already closed or not started
	}

	if s.ctx.Err() != nil {
		return s.ctx.Err() // Context already cancelled
	}

	// Cancel the context to prevent new operations from being enqueued
	s.cancel()

	// Close the split channel to signal workers to stop
	close(s.splitCh)
	close(s.mergeCh)
	close(s.reassignCh)

	s.wg.Wait() // Wait for all workers to finish
	return nil
}

// deduplicator ensures only one operation per posting ID can be enqueued at a time.
type deduplicator struct {
	inflight *xsync.Map[uint64, struct{}] // map of inflight operations by posting ID
}

func newDeduplicator() *deduplicator {
	return &deduplicator{
		inflight: xsync.NewMap[uint64, struct{}](),
	}
}

func (d *deduplicator) tryEnqueue(id uint64) bool {
	_, exists := d.inflight.LoadAndStore(id, struct{}{})
	return !exists
}

func (d *deduplicator) done(id uint64) {
	d.inflight.Delete(id)
}

func (d *deduplicator) contains(id uint64) bool {
	_, exists := d.inflight.Load(id)
	return exists
}
