//                           _       _
// __      _____  __ ___   ___  __ _| |_ ___
// \ \ /\ / / _ \/ _` \ \ / / |/ _` | __/ _ \
//  \ V  V /  __/ (_| |\ V /| | (_| | ||  __/
//   \_/\_/ \___|\__,_| \_/ |_|\__,_|\__\___|
//
//  Copyright Â© 2016 - 2025 Weaviate B.V. All rights reserved.
//
//  CONTACT: hello@weaviate.io
//

package spfresh

import (
	"context"
	"sync"

	"github.com/pkg/errors"
	"github.com/puzpuzpuz/xsync/v4"
	"github.com/sirupsen/logrus"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/common"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/compressionhelpers"
	enterrors "github.com/weaviate/weaviate/entities/errors"
)

const (
	reassignThreshold = 3        // Fine-tuned threshold to avoid unnecessary splits during reassign operations
	splitReuseEpsilon = 0.000001 // Epsilon to determine if a split can reuse the existing posting
)

var (
	ErrPostingNotFound  = errors.New("posting not found")
	ErrVectorNotFound   = errors.New("vector not found")
	ErrIdenticalVectors = errors.New("posting list contains identical or near-identical vectors")
)

type SplitOperation struct {
	PostingID uint64
}

type MergeOperation struct {
	PostingID uint64
}

type ReassignOperation struct {
	PostingID uint64
	Vector    Vector
}

// SPFresh manages background operations for postings in the SPFresh index.
// It handles split, merge, and reassign requests generated by insertions or during search
// operations. It uses a worker pool to process these operations concurrently.
type SPFresh struct {
	Logger       *logrus.Entry
	UserConfig   *UserConfig                      // UserConfig contains user-defined settings for the rebuilder.
	SPTAG        SPTAG                            // SPTAG provides access to the SPTAG index for centroid operations.
	Store        *LSMStore                        // Used for managing persistence of postings.
	Splitter     PostingSplitter                  // Used for splitting postings into two.
	VersionMap   *VersionMap                      // Provides access to vector versions.
	IDs          *common.MonotonicCounter[uint64] // Shared monotonic counter for generating unique IDs for new postings.
	PostingSizes *PostingSizes                    // Stores the size of each posting
	Quantizer    *compressionhelpers.RotationalQuantizer

	// ctx and cancel are used to manage the lifecycle of the LocalRebuilder.
	ctx    context.Context
	cancel context.CancelFunc

	splitCh    chan SplitOperation    // Channel for split operations
	mergeCh    chan MergeOperation    // Channel for merge operations
	reassignCh chan ReassignOperation // Channel for reassign operations
	wg         sync.WaitGroup

	splitList *deduplicator // Prevents duplicate split operations
	mergeList *deduplicator // Prevents duplicate merge operations

	postingLocks *common.HashedLocks // Locks to prevent concurrent modifications to the same posting.
}

func (s *SPFresh) Start(ctx context.Context) {
	if s.UserConfig == nil {
		panic("UserConfig must be set before starting LocalRebuilder")
	}
	if s.Store == nil {
		panic("Store must be set before starting LocalRebuilder")
	}
	if s.Splitter == nil {
		panic("Splitter must be set before starting LocalRebuilder")
	}
	if s.VersionMap == nil {
		panic("VersionMap must be set before starting LocalRebuilder")
	}
	if s.IDs == nil {
		panic("IdGenerator must be set before starting LocalRebuilder")
	}
	if s.Quantizer == nil {
		panic("Quantizer must be set before starting LocalRebuilder")
	}

	s.ctx, s.cancel = context.WithCancel(ctx)

	if s.Logger == nil {
		s.Logger = logrus.New().WithField("component", "LocalRebuilder")
	} else {
		s.Logger = s.Logger.WithField("component", "LocalRebuilder")
	}

	s.postingLocks = common.NewHashedLocks32k()
	s.splitCh = make(chan SplitOperation, s.UserConfig.SplitWorkers*100)          // TODO: fine-tune buffer size
	s.mergeCh = make(chan MergeOperation, 1024)                                   // TODO: fine-tune buffer size
	s.reassignCh = make(chan ReassignOperation, s.UserConfig.ReassignWorkers*100) // TODO: fine-tune buffer size

	// start N workers to process split operations
	for i := 0; i < s.UserConfig.SplitWorkers; i++ {
		s.wg.Add(1)
		enterrors.GoWrapper(s.splitWorker, s.Logger)
	}

	// start M workers to process reassign operations
	for i := 0; i < s.UserConfig.ReassignWorkers; i++ {
		s.wg.Add(1)
		enterrors.GoWrapper(s.reassignWorker, s.Logger)
	}

	// start a single worker to process merge operations
	s.wg.Add(1)
	enterrors.GoWrapper(s.mergeWorker, s.Logger)
}

func (s *SPFresh) Append(id uint64, vector []byte, reassigned bool) (bool, error) {
	// search the nearest centroid
	ps, err := s.SPTAG.Search(vector, 1)
	if err != nil {
		return false, errors.Wrap(err, "failed to search for nearest centroid")
	}

	// register the vector in the version map
	s.VersionMap.AllocPageFor(id)
	version, _ := s.VersionMap.Increment(id)

	v := Vector{
		ID:      id,
		Version: version,
		Data:    vector,
	}

	var postingID uint64

	// if there are no postings, create a new one
	if len(ps) == 0 {
		postingID = s.IDs.Next()

		// persist the new posting first
		err = s.Store.Put(s.ctx, postingID, Posting{v})
		if err != nil {
			return false, errors.Wrapf(err, "failed to persist new posting %d", postingID)
		}

		// use the vector as the centroid and register it in the SPTAG
		err = s.SPTAG.Upsert(postingID, vector)
		if err != nil {
			return false, errors.Wrapf(err, "failed to upsert new centroid %d", postingID)
		}
	} else {
		postingID = ps[0].ID

		// lock the posting to ensure no concurrent modifications
		s.postingLocks.Lock(postingID)

		// check if the posting still exists
		if !s.SPTAG.Exists(postingID) {
			// the vector might have been deleted concurrently,
			// might happen if we are reassigning
			if s.VersionMap.Get(id) == version {
				err = s.enqueueReassign(s.ctx, postingID, v)
				if err != nil {
					return false, errors.Wrapf(err, "failed to enqueue reassign for vector %d", id)
				}
			}

			return false, nil
		}

		// append the new vector to the existing posting
		err = s.Store.Merge(s.ctx, postingID, v)
		if err != nil {
			s.postingLocks.Unlock(postingID)
			return false, err
		}

		// increment the size of the posting
		s.PostingSizes.Inc(postingID, 1)

		s.postingLocks.Unlock(postingID)
	}

	// ensure the posting size is within the configured limits
	count := s.PostingSizes.Get(postingID)

	// If the posting is too big, we need to split it.
	// During an insert, we want to split asynchronously
	// however during a reassign, we want to split immediately.
	// Also, reassign operations may cause the posting to grow beyond the max size
	// temporarily. To avoid triggering unnecessary splits, we add a fine-tuned threshold.
	max := s.UserConfig.MaxPostingSize
	if reassigned {
		max += reassignThreshold
	}
	if count > max {
		if reassigned {
			err = s.doSplit(postingID)
		} else {
			err = s.EnqueueSplit(s.ctx, postingID)
		}
		if err != nil {
			return false, err
		}
	}

	return true, nil
}

// Delete marks a vector as deleted in the version map.
func (s *SPFresh) Delete(ctx context.Context, id uint64) error {
	version := s.VersionMap.MarkDeleted(id)
	if version == 0 {
		return ErrVectorNotFound
	}

	return nil
}

func (s *SPFresh) EnqueueSplit(ctx context.Context, postingID uint64) error {
	if s.ctx == nil {
		return nil // Not started yet
	}

	// Check if the operation is already in progress
	if !s.splitList.tryEnqueue(postingID) {
		s.Logger.WithField("postingID", postingID).
			Debug("Split operation already enqueued, skipping")
		return nil
	}

	// Enqueue the operation to the channel
	select {
	case s.splitCh <- SplitOperation{PostingID: postingID}:
	case <-ctx.Done():
		return ctx.Err()
	}

	return nil
}

func (s *SPFresh) EnqueueMerge(ctx context.Context, postingID uint64) error {
	if s.ctx == nil {
		return nil // Not started yet
	}

	// Check if the operation is already in progress
	if !s.mergeList.tryEnqueue(postingID) {
		s.Logger.WithField("postingID", postingID).
			Debug("Merge operation already enqueued, skipping")
		return nil
	}

	// Enqueue the operation to the channel
	select {
	case s.mergeCh <- MergeOperation{PostingID: postingID}:
	case <-ctx.Done():
		return ctx.Err()
	}

	return nil
}

func (s *SPFresh) enqueueReassign(ctx context.Context, postingID uint64, vector Vector) error {
	if s.ctx == nil {
		return nil // Not started yet
	}

	// Enqueue the operation to the channel
	select {
	case s.reassignCh <- ReassignOperation{PostingID: postingID, Vector: vector}:
	case <-ctx.Done():
		return ctx.Err()
	}

	return nil
}

func (s *SPFresh) splitWorker() {
	defer s.wg.Done()

	for op := range s.splitCh {
		if s.ctx.Err() != nil {
			return
		}

		err := s.doSplit(op.PostingID)
		if err != nil {
			s.Logger.WithError(err).
				WithField("postingID", op.PostingID).
				Error("Failed to process split operation")
			continue // Log the error and continue processing other operations
		}
	}
}

func (s *SPFresh) mergeWorker() {
	defer s.wg.Done()

	for op := range s.mergeCh {
		if s.ctx.Err() != nil {
			return // Exit if the context is cancelled
		}

		err := s.doMerge(op)
		if err != nil {
			s.Logger.WithError(err).
				WithField("postingID", op.PostingID).
				Error("Failed to process merge operation")
			continue // Log the error and continue processing other operations
		}
	}
}

func (s *SPFresh) reassignWorker() {
	defer s.wg.Done()

	for op := range s.reassignCh {
		if s.ctx.Err() != nil {
			return
		}

		err := s.doReassign(op)
		if err != nil {
			s.Logger.WithError(err).
				WithField("vectorID", op.Vector.ID).
				Error("Failed to process reassign operation")
			continue // Log the error and continue processing other operations
		}
	}
}

func (s *SPFresh) Close(ctx context.Context) error {
	if s.ctx == nil {
		return nil // Already closed or not started
	}

	if s.ctx.Err() != nil {
		return s.ctx.Err() // Context already cancelled
	}

	// Cancel the context to prevent new operations from being enqueued
	s.cancel()

	// Close the split channel to signal workers to stop
	close(s.splitCh)
	close(s.mergeCh)
	close(s.reassignCh)

	s.wg.Wait() // Wait for all workers to finish
	return nil
}

func (s *SPFresh) doSplit(postingID uint64) error {
	s.postingLocks.Lock(postingID)

	var markedAsDone bool
	defer func() {
		if !markedAsDone {
			s.postingLocks.Unlock(postingID)
			s.splitList.done(postingID)
		}
	}()

	p, err := s.Store.Get(s.ctx, postingID)
	if err != nil {
		if err == ErrPostingNotFound {
			s.Logger.WithField("postingID", postingID).
				Debug("Posting not found, skipping split operation")
			return nil
		}

		return errors.Wrapf(err, "failed to get posting %d for split operation", postingID)
	}

	// garbage collect the deleted vectors
	filtered := p.GarbageCollect(s.VersionMap)

	// skip if the filtered posting is now too small
	if len(filtered) < int(s.UserConfig.MaxPostingSize) {
		s.Logger.
			WithField("postingID", postingID).
			WithField("size", len(filtered)).
			WithField("max", s.UserConfig.MaxPostingSize).
			Debug("Posting has less than max size after garbage collection, skipping split operation")

		if len(filtered) == len(p) {
			// no changes, just return
			return nil
		}

		// update the size of the posting
		// Note: the page associated to this posting is guaranteed to exist in the
		// postingSizes. no need to check for existence.
		s.PostingSizes.Set(postingID, uint32(len(filtered)))

		// persist the gc'ed posting
		err = s.Store.Put(s.ctx, postingID, filtered)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d after split operation", postingID)
		}

		return nil
	}

	// split the vectors into two clusters
	result, err := s.Splitter.Split(filtered)
	if err != nil || len(result) < 2 {
		if !errors.Is(err, ErrIdenticalVectors) {
			return errors.Wrapf(err, "failed to split vectors for posting %d", postingID)
		}

		// If the split fails because the posting contains identical vectors,
		// we override the posting with a single vector
		s.Logger.WithField("postingID", postingID).
			WithError(err).
			Debug("Cannot split posting: contains identical vectors, keeping only one vector")

		s.PostingSizes.Set(postingID, 1)

		err = s.Store.Put(s.ctx, postingID, Posting{filtered[0]})
		if err != nil {
			return errors.Wrapf(err, "failed to put single vector posting %d after split operation", postingID)
		}

		return nil
	}

	newPostingIDs := make([]uint64, 2)
	var postingReused bool
	for i := range 2 {
		// if the centroid of the existing posting is close enough to one of the new centroids
		// we can reuse the existing posting
		if !postingReused {
			existingCentroid := s.SPTAG.Get(postingID)
			dist, err := s.SPTAG.ComputeDistance(existingCentroid, result[i].Centroid)
			if err != nil {
				return errors.Wrapf(err, "failed to compute distance for split operation on posting %d", postingID)
			}

			if dist < splitReuseEpsilon {
				postingReused = true
				newPostingIDs[i] = postingID
				s.PostingSizes.Set(postingID, uint32(len(result[i].Posting)))
				err = s.Store.Put(s.ctx, postingID, result[i].Posting)
				if err != nil {
					return errors.Wrapf(err, "failed to put reused posting %d after split operation", postingID)
				}

				continue
			}
		}

		// otherwise, we need to create a new posting for the new centroid
		newPostingID := s.IDs.Next()
		newPostingIDs[i] = newPostingID
		err = s.Store.Put(s.ctx, newPostingID, result[i].Posting)
		if err != nil {
			return errors.Wrapf(err, "failed to put new posting %d after split operation", newPostingID)
		}
		s.PostingSizes.AllocPageFor(newPostingID)
		s.PostingSizes.Set(newPostingID, uint32(len(result[i].Posting)))

		// add the new centroid to the SPTAG index
		err = s.SPTAG.Upsert(newPostingID, result[i].Centroid)
		if err != nil {
			return errors.Wrapf(err, "failed to upsert new centroid %d after split operation", newPostingID)
		}
	}

	if !postingReused {
		err = s.SPTAG.Delete(postingID)
		if err != nil {
			return errors.Wrapf(err, "failed to delete old centroid %d after split operation", postingID)
		}
		s.PostingSizes.Set(postingID, 0) // Mark the old posting as deleted
	}

	// Mark the split operation as done
	markedAsDone = true
	s.postingLocks.Unlock(postingID)
	s.splitList.done(postingID)

	err = s.enqueueReassignAfterSplit(postingID, newPostingIDs, result)
	if err != nil {
		return errors.Wrapf(err, "failed to enqueue reassign after split for posting %d", postingID)
	}

	return nil
}

func (s *SPFresh) enqueueReassignAfterSplit(oldPostingID uint64, newPostingIDs []uint64, newPostings []SplitResult) error {
	oldCentroid := s.SPTAG.Get(oldPostingID)

	reassignedVectors := make(map[uint64]struct{})

	// first check: if a vector is closer to one of the new posting centroid than the old centroid,
	// neighboring centroids cannot be better.
	for i := range newPostings {
		// test each vector
		for _, v := range newPostings[i].Posting {
			_, exists := reassignedVectors[v.ID]
			if !exists && !v.Version.Deleted() && s.VersionMap.Get(v.ID) == v.Version {
				// compute distance from v to its new centroid
				newDist, err := s.SPTAG.ComputeDistance(s.SPTAG.Get(newPostingIDs[i]), v.Data)
				if err != nil {
					return errors.Wrapf(err, "failed to compute distance for vector %d in new posting %d", v.ID, newPostingIDs[i])
				}

				// compute distance from v to the old centroid
				oldDist, err := s.SPTAG.ComputeDistance(oldCentroid, v.Data)
				if err != nil {
					return errors.Wrapf(err, "failed to compute distance for vector %d in old posting %d", v.ID, oldPostingID)
				}

				if newDist >= oldDist {
					// the vector is closer to the old centroid, which means it may be also closer to a neighboring centroid,
					// we need to reassign it
					err = s.enqueueReassign(s.ctx, newPostingIDs[i], v)
					if err != nil {
						return errors.Wrapf(err, "failed to enqueue reassign for vector %d after split", v.ID)
					}
					reassignedVectors[v.ID] = struct{}{}
				}
			}
		}
	}

	// second check: if a vector from a neighboring centroid is closer to one of the new posting centroids than the old centroid,
	// we need to reassign it.
	if s.UserConfig.ReassignNeighbors <= 0 {
		return nil
	}

	// search for neighboring centroids
	nearest, err := s.SPTAG.Search(oldCentroid, s.UserConfig.ReassignNeighbors)
	if err != nil {
		return errors.Wrapf(err, "failed to search for nearest centroids for reassign after split for posting %d", oldPostingID)
	}

	seen := make(map[uint64]struct{})
	for _, id := range newPostingIDs {
		seen[id] = struct{}{}
	}
	// for each neighboring centroid, check if any of its vectors is closer to one of the new centroids
	for _, neighbor := range nearest {
		_, exists := seen[neighbor.ID]
		if exists {
			continue
		}
		seen[neighbor.ID] = struct{}{}

		p, err := s.Store.Get(s.ctx, neighbor.ID)
		if err != nil {
			if errors.Is(err, ErrPostingNotFound) {
				s.Logger.WithField("postingID", neighbor.ID).
					Debug("Posting not found, skipping reassign after split")
				continue // Skip if the posting is not found
			}

			return errors.Wrapf(err, "failed to get posting %d for reassign after split", neighbor.ID)
		}

		for _, v := range p {
			_, exists := reassignedVectors[v.ID]
			if !exists && !v.Version.Deleted() && s.VersionMap.Get(v.ID) == v.Version {
				distNeighbor, err := s.SPTAG.ComputeDistance(s.SPTAG.Get(neighbor.ID), v.Data)
				if err != nil {
					return errors.Wrapf(err, "failed to compute distance for vector %d in neighbor posting %d", v.ID, neighbor.ID)
				}

				distOld, err := s.SPTAG.ComputeDistance(oldCentroid, v.Data)
				if err != nil {
					return errors.Wrapf(err, "failed to compute distance for vector %d in old posting %d", v.ID, oldPostingID)
				}

				distA0, err := s.SPTAG.ComputeDistance(s.SPTAG.Get(newPostingIDs[0]), v.Data)
				if err != nil {
					return errors.Wrapf(err, "failed to compute distance for vector %d in new posting %d", v.ID, newPostingIDs[0])
				}

				distA1, err := s.SPTAG.ComputeDistance(s.SPTAG.Get(newPostingIDs[1]), v.Data)
				if err != nil {
					return errors.Wrapf(err, "failed to compute distance for vector %d in new posting %d", v.ID, newPostingIDs[1])
				}

				if distOld <= distA0 && distOld <= distA1 {
					// the vector is closer to the old centroid, which means the new postings are not better than its current posting
					continue
				}

				if distNeighbor < distA0 && distNeighbor < distA1 {
					// the vector is closer to its current centroid than to the new centroids,
					// no need to reassign it
					continue
				}

				// the vector is closer to one of the new centroids, it needs to be reassigned
				err = s.enqueueReassign(s.ctx, neighbor.ID, v)
				if err != nil {
					return errors.Wrapf(err, "failed to enqueue reassign for vector %d after split", v.ID)
				}
				reassignedVectors[v.ID] = struct{}{}
			}
		}
	}

	return nil
}

func (s *SPFresh) doMerge(op MergeOperation) error {
	defer s.mergeList.done(op.PostingID)

	var markedAsDone bool
	s.postingLocks.Lock(op.PostingID)
	defer func() {
		if !markedAsDone {
			s.postingLocks.Unlock(op.PostingID)
		}
	}()

	// Ensure the posting exists in the index
	if !s.SPTAG.Exists(op.PostingID) {
		s.Logger.WithField("postingID", op.PostingID).
			Debug("Posting not found, skipping merge operation")
		return nil // Nothing to merge
	}

	p, err := s.Store.Get(s.ctx, op.PostingID)
	if err != nil {
		if err == ErrPostingNotFound {
			s.Logger.WithField("postingID", op.PostingID).
				Debug("Posting not found, skipping merge operation")
			return nil
		}

		return errors.Wrapf(err, "failed to get posting %d for merge operation", op.PostingID)
	}

	// garbage collect the deleted vectors
	vectorSet := make(map[uint64]struct{})
	merged := make(Posting, 0, len(p))
	for _, v := range p {
		version := s.VersionMap.Get(v.ID)
		if version.Deleted() || version.Version() > v.Version.Version() {
			continue
		}
		merged = append(merged, Vector{
			ID:      v.ID,
			Version: version,
			Data:    v.Data,
		})
		vectorSet[v.ID] = struct{}{}
	}
	prevLen := len(merged)

	// skip if the posting is big enough
	if len(merged) >= int(s.UserConfig.MinPostingSize) {
		s.Logger.
			WithField("postingID", op.PostingID).
			WithField("size", len(merged)).
			WithField("min", s.UserConfig.MinPostingSize).
			Debug("Posting is big enough, skipping merge operation")

		if len(merged) == len(p) {
			// no changes, just return
			return nil
		}

		// update the size of the posting
		s.PostingSizes.Set(op.PostingID, uint32(len(merged)))

		// persist the gc'ed posting
		err = s.Store.Put(s.ctx, op.PostingID, merged)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d", op.PostingID)
		}

		return nil
	}

	// get posting centroid
	oldCentroid := s.SPTAG.Get(op.PostingID)
	if oldCentroid == nil {
		return errors.Errorf("centroid not found for posting %d", op.PostingID)
	}

	// search for the closest centroids
	nearest, err := s.SPTAG.Search(oldCentroid, s.UserConfig.InternalPostingCandidates)
	if err != nil {
		return errors.Wrapf(err, "failed to search for nearest centroid for posting %d", op.PostingID)
	}

	if len(nearest) <= 1 {
		s.Logger.WithField("postingID", op.PostingID).
			Debug("No candidates found for merge operation, skipping")

		// update the size of the posting
		s.PostingSizes.Set(op.PostingID, uint32(len(merged)))

		// persist the gc'ed posting
		err = s.Store.Put(s.ctx, op.PostingID, merged)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d", op.PostingID)
		}

		return nil
	}

	// first centroid is the query centroid, the rest are candidates for merging
	for i := 1; i < len(nearest); i++ {
		// check if the combined size of the postings is within limits
		count := s.PostingSizes.Get(nearest[i].ID)
		if int(count)+len(merged) > int(s.UserConfig.MaxPostingSize) || s.mergeList.contains(nearest[i].ID) {
			continue // Skip this candidate
		}

		// lock the candidate posting to ensure no concurrent modifications
		// note: the candidate lock might be the same as the current posting lock
		// so we need to ensure we don't deadlock
		if s.postingLocks.Hash(op.PostingID) != s.postingLocks.Hash(nearest[i].ID) {
			// lock the candidate posting
			s.postingLocks.Lock(nearest[i].ID)
			defer func() {
				if !markedAsDone {
					s.postingLocks.Unlock(nearest[i].ID)
				}
			}()
		}

		// get the candidate posting
		candidate, err := s.Store.Get(s.ctx, nearest[i].ID)
		if err != nil {
			return errors.Wrapf(err, "failed to get candidate posting %d for merge operation on posting %d", nearest[i], op.PostingID)
		}

		var candidateLen int
		for _, v := range candidate {
			version := s.VersionMap.Get(v.ID)
			if version.Deleted() || version.Version() > v.Version.Version() {
				continue
			}
			if _, exists := vectorSet[v.ID]; exists {
				continue // Skip duplicate vectors
			}
			merged = append(merged, Vector{
				ID:      v.ID,
				Version: version,
				Data:    v.Data,
			})
			candidateLen++
		}

		// delete the smallest posting and update the large posting
		smallID, largeID := op.PostingID, nearest[i].ID
		smallPosting := p
		if prevLen > candidateLen {
			smallID, largeID = nearest[i].ID, op.PostingID
			smallPosting = candidate
		}

		err = s.SPTAG.Delete(smallID)
		if err != nil {
			return errors.Wrapf(err, "failed to delete centroid for posting %d", smallID)
		}

		err = s.Store.Put(s.ctx, largeID, merged)
		if err != nil {
			return errors.Wrapf(err, "failed to put merged posting %d after merge operation", op.PostingID)
		}

		// set the small posting size to 0 and update the large posting size
		s.PostingSizes.Set(smallID, 0)
		s.PostingSizes.Set(largeID, uint32(len(merged)))

		// mark the operation as done and unlock everything
		markedAsDone = true
		if s.postingLocks.Hash(op.PostingID) != s.postingLocks.Hash(nearest[i].ID) {
			s.postingLocks.Unlock(nearest[i].ID)
		}
		s.postingLocks.Unlock(op.PostingID)

		// if merged vectors are closer to their old centroid than the new one
		// there may be better centroids for them out there.
		// we need to reassign them in the background.
		smallCentroid := s.SPTAG.Get(smallID)
		largeCentroid := s.SPTAG.Get(largeID)
		for _, v := range smallPosting {
			prevDist, err := s.SPTAG.ComputeDistance(smallCentroid, v.Data)
			if err != nil {
				return errors.Wrapf(err, "failed to compute distance for vector %d in small posting %d", v.ID, smallID)
			}

			newDist, err := s.SPTAG.ComputeDistance(largeCentroid, v.Data)
			if err != nil {
				return errors.Wrapf(err, "failed to compute distance for vector %d in large posting %d", v.ID, largeID)
			}

			if prevDist < newDist {
				// the vector is closer to the old centroid, we need to reassign it
				err = s.enqueueReassign(s.ctx, largeID, v)
				if err != nil {
					return errors.Wrapf(err, "failed to enqueue reassign for vector %d after merge", v.ID)
				}
			}
		}

		return nil
	}

	// if no candidates were found, just persist the gc'ed posting
	s.PostingSizes.Set(op.PostingID, uint32(len(merged)))
	err = s.Store.Put(s.ctx, op.PostingID, merged)
	if err != nil {
		return errors.Wrapf(err, "failed to put filtered posting %d", op.PostingID)
	}

	return nil
}

func (s *SPFresh) doReassign(op ReassignOperation) error {
	// check if the vector is still valid
	version := s.VersionMap.Get(op.Vector.ID)
	if version.Deleted() || version.Version() > op.Vector.Version.Version() {
		s.Logger.WithField("vectorID", op.Vector.ID).
			Debug("Vector is deleted or has a newer version, skipping reassign operation")
		return nil
	}

	// perform a RNG selection to determine the postings where the vector should be
	// reassigned to.
	replicas, needsReassign, err := s.selectReplicas(op.Vector, op.PostingID)
	if err != nil {
		return errors.Wrap(err, "failed to select replicas")
	}
	if !needsReassign {
		s.Logger.WithField("vectorID", op.Vector.ID).
			Debug("Vector is already assigned to the best posting, skipping reassign operation")
		return nil
	}
	// check again if the version is still valid
	version = s.VersionMap.Get(op.Vector.ID)
	if version.Deleted() || version.Version() > op.Vector.Version.Version() {
		s.Logger.WithField("vectorID", op.Vector.ID).
			Debug("Vector is deleted or has a newer version, skipping reassign operation")
		return nil
	}

	// append the vector to each replica
	for _, replica := range replicas {
		ok, err := s.Append(replica, op.Vector.Data, true)
		if !ok {
			s.Logger.WithField("vectorID", op.Vector.ID).
				WithField("replicaID", replica).
				Debug("Posting no longer exists, reassigning")
			continue // Skip if the vector already exists in the replica
		}
		if err != nil {
			return err
		}
	}

	return nil
}

func (s *SPFresh) selectReplicas(query Vector, unless uint64) ([]uint64, bool, error) {
	results, err := s.SPTAG.Search(query.Data, s.UserConfig.InternalPostingCandidates)
	if err != nil {
		return nil, false, errors.Wrap(err, "failed to search for nearest neighbors")
	}

	replicas := make([]uint64, 0, s.UserConfig.Replicas)

LOOP:
	for i := 0; i < len(results) && len(replicas) < s.UserConfig.Replicas; i++ {
		candidate := results[i]

		// determine if the candidate is too close to a pre-existing replica
		for j := range replicas {
			dist, err := s.SPTAG.ComputeDistance(s.SPTAG.Get(results[i].ID), s.SPTAG.Get(replicas[j]))
			if err != nil {
				return nil, false, errors.Wrapf(err, "failed to compute distance for edge %d -> %d", results[i].ID, replicas[j])
			}

			if s.UserConfig.RNGFactor*dist <= candidate.Distance {
				continue LOOP
			}
		}

		// if unless is specified, abort the RNG selection if one of the replicas
		// is the unless posting ID (i.e. the vector is already assigned to this posting)
		if unless != 0 && candidate.ID == unless {
			return nil, false, nil
		}

		replicas = append(replicas, candidate.ID)
	}

	return replicas, true, nil
}

// deduplicator ensures only one operation per posting ID can be enqueued at a time.
type deduplicator struct {
	inflight *xsync.Map[uint64, struct{}] // map of inflight operations by posting ID
}

func newDeduplicator() *deduplicator {
	return &deduplicator{
		inflight: xsync.NewMap[uint64, struct{}](),
	}
}

func (d *deduplicator) tryEnqueue(id uint64) bool {
	_, exists := d.inflight.LoadAndStore(id, struct{}{})
	return !exists
}

func (d *deduplicator) done(id uint64) {
	d.inflight.Delete(id)
}

func (d *deduplicator) contains(id uint64) bool {
	_, exists := d.inflight.Load(id)
	return exists
}
