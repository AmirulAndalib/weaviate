//                           _       _
// __      _____  __ ___   ___  __ _| |_ ___
// \ \ /\ / / _ \/ _` \ \ / / |/ _` | __/ _ \
//  \ V  V /  __/ (_| |\ V /| | (_| | ||  __/
//   \_/\_/ \___|\__,_| \_/ |_|\__,_|\__\___|
//
//  Copyright Â© 2016 - 2025 Weaviate B.V. All rights reserved.
//
//  CONTACT: hello@weaviate.io
//

package spfresh

import (
	"context"
	"sync"

	"github.com/pkg/errors"
	"github.com/puzpuzpuz/xsync/v4"
	"github.com/sirupsen/logrus"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/common"
	enterrors "github.com/weaviate/weaviate/entities/errors"
)

type SplitOperation struct {
	PostingID uint64
}

type MergeOperation struct {
	PostingID uint64
}

// LocalRebuilder manages background operations for postings in the SPFresh index.
// It handles split, merge, and reassign requests generated by insertions or during search
// operations. It uses a worker pool to process these operations concurrently.
type LocalRebuilder struct {
	Logger       *logrus.Entry
	UserConfig   *UserConfig                      // UserConfig contains user-defined settings for the rebuilder.
	SPTAG        SPTAG                            // SPTAG provides access to the SPTAG index for centroid operations.
	Store        *LSMStore                        // Used for managing persistence of postings.
	Splitter     PostingSplitter                  // Used for splitting postings into two.
	VersionMap   *VersionMap                      // Provides access to vector versions.
	IDs          *common.MonotonicCounter[uint64] // Shared monotonic counter for generating unique IDs for new postings.
	PostingSizes *PostingSizes                    // Stores the size of each posting

	// ctx and cancel are used to manage the lifecycle of the LocalRebuilder.
	ctx    context.Context
	cancel context.CancelFunc

	splitCh chan SplitOperation               // Channel for split operations
	mergeCh *xsync.UMPSCQueue[MergeOperation] // Unbounded channel for merge operations
	wg      sync.WaitGroup

	splitList *deduplicator // Prevents duplicate split operations
	mergeList *deduplicator // Prevents duplicate merge operations

	postingLocks *common.HashedLocks // Locks to prevent concurrent modifications to the same posting.
}

func (l *LocalRebuilder) Start(ctx context.Context) {
	if l.UserConfig == nil {
		panic("UserConfig must be set before starting LocalRebuilder")
	}
	if l.Store == nil {
		panic("Store must be set before starting LocalRebuilder")
	}
	if l.Splitter == nil {
		panic("Splitter must be set before starting LocalRebuilder")
	}
	if l.VersionMap == nil {
		panic("VersionMap must be set before starting LocalRebuilder")
	}
	if l.IDs == nil {
		panic("IdGenerator must be set before starting LocalRebuilder")
	}

	l.ctx, l.cancel = context.WithCancel(context.Background())

	if l.Logger == nil {
		l.Logger = logrus.New().WithField("component", "LocalRebuilder")
	} else {
		l.Logger = l.Logger.WithField("component", "LocalRebuilder")
	}

	l.postingLocks = common.NewHashedLocks32k()
	l.splitCh = make(chan SplitOperation, l.UserConfig.SplitWorkers*100) // TODO: fine-tune buffer size
	l.mergeCh = xsync.NewUMPSCQueue[MergeOperation]()

	// start N workers to process split operations
	for i := 0; i < l.UserConfig.SplitWorkers; i++ {
		l.wg.Add(1)
		enterrors.GoWrapper(l.splitWorker, l.Logger)
	}

	// start a single worker to process merge operations
	l.wg.Add(1)
	enterrors.GoWrapper(l.mergeWorker, l.Logger)
}

func (l *LocalRebuilder) EnqueueSplit(ctx context.Context, postingID uint64) error {
	if l.ctx == nil {
		return nil // Not started yet
	}

	if l.ctx.Err() != nil {
		return l.ctx.Err() // Context already cancelled
	}

	// Check if the operation is already in progress
	if !l.splitList.tryEnqueue(postingID) {
		l.Logger.WithField("postingID", postingID).
			Debug("Split operation already enqueued, skipping")
		return nil
	}

	// Enqueue the operation to the channel
	select {
	case l.splitCh <- SplitOperation{PostingID: postingID}:
	case <-ctx.Done():
		return ctx.Err()
	}

	return nil
}

func (l *LocalRebuilder) EnqueueMerge(ctx context.Context, postingID uint64) error {
	if l.ctx == nil {
		return nil // Not started yet
	}

	if l.ctx.Err() != nil {
		return l.ctx.Err() // Context already cancelled
	}

	// Check if the operation is already in progress
	if !l.mergeList.tryEnqueue(postingID) {
		l.Logger.WithField("postingID", postingID).
			Debug("Merge operation already enqueued, skipping")
		return nil
	}

	// Enqueue the operation to the channel
	l.mergeCh.Enqueue(MergeOperation{PostingID: postingID})

	return nil
}

func (l *LocalRebuilder) splitWorker() {
	defer l.wg.Done()

	for op := range l.splitCh {
		if l.ctx.Err() != nil {
			return
		}

		err := l.doSplit(op.PostingID)
		if err != nil {
			l.Logger.WithError(err).
				WithField("postingID", op.PostingID).
				Error("Failed to process split operation")
			continue // Log the error and continue processing other operations
		}
	}
}

func (l *LocalRebuilder) mergeWorker() {
	defer l.wg.Done()

	for {
		op := l.mergeCh.Dequeue()

		if l.ctx.Err() != nil {
			return // Exit if the context is cancelled
		}

		// A zero-value MergeOperation indicates no more operations to process
		if op.PostingID == 0 {
			// No more merge operations to process
			return
		}

		err := l.doMerge(op)
		if err != nil {
			l.Logger.WithError(err).
				WithField("postingID", op.PostingID).
				Error("Failed to process merge operation")
			continue // Log the error and continue processing other operations
		}
	}
}

func (l *LocalRebuilder) Close(ctx context.Context) error {
	if l.ctx == nil {
		return nil // Already closed or not started
	}

	if l.ctx.Err() != nil {
		return l.ctx.Err() // Context already cancelled
	}

	// Cancel the context to prevent new operations from being enqueued
	l.cancel()

	// Close the split channel to signal workers to stop
	close(l.splitCh)

	// Close the merge channel by enqueuing a zero-value MergeOperation
	l.mergeCh.Enqueue(MergeOperation{})

	l.wg.Wait() // Wait for all workers to finish
	return nil
}

func (l *LocalRebuilder) doSplit(postingID uint64) error {
	defer l.splitList.done(postingID) // Ensure we mark the operation as done

	l.postingLocks.Lock(postingID)
	defer l.postingLocks.Unlock(postingID)

	p, err := l.Store.Get(l.ctx, postingID)
	if err != nil {
		if err == ErrPostingNotFound {
			l.Logger.WithField("postingID", postingID).
				Debug("Posting not found, skipping split operation")
			return nil
		}

		return errors.Wrapf(err, "failed to get posting %d for split operation", postingID)
	}

	// garbage collect the deleted vectors
	filtered := p.GarbageCollect(l.VersionMap)

	// skip if the filtered posting is now too small
	if len(filtered) < l.UserConfig.MaxPostingSize {
		l.Logger.
			WithField("postingID", postingID).
			WithField("size", len(filtered)).
			WithField("max", l.UserConfig.MaxPostingSize).
			Debug("Posting has less than max size after garbage collection, skipping split operation")

		if len(filtered) == len(p) {
			// no changes, just return
			return nil
		}

		// update the size of the posting
		// Note: the page associated to this posting is guaranteed to exist in the
		// postingSizes. no need to check for existence.
		l.PostingSizes.Set(postingID, uint32(len(filtered)))

		// persist the gc'ed posting
		err = l.Store.Put(l.ctx, postingID, filtered)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d after split operation", postingID)
		}

		return nil
	}

	// split the vectors into two clusters
	result, err := l.Splitter.Split(filtered)
	if err != nil {
		return errors.Wrapf(err, "failed to split vectors for posting %d", postingID)
	}

	// persist the new postings first
	leftID, rightID := l.IDs.NextN(2)
	err = l.Store.Put(l.ctx, leftID, result.LeftPosting)
	if err != nil {
		return errors.Wrapf(err, "failed to put left posting %d for split operation on posting %d", leftID, postingID)
	}
	err = l.Store.Put(l.ctx, rightID, result.RightPosting)
	if err != nil {
		// cleanup will be handled by the snapshot process
		return errors.Wrapf(err, "failed to put right posting %d for split operation on posting %d", rightID, postingID)
	}

	// register the new posting sizes:
	l.PostingSizes.AllocPageFor(leftID, rightID)
	l.PostingSizes.Set(leftID, uint32(len(result.LeftPosting)))
	l.PostingSizes.Set(rightID, uint32(len(result.RightPosting)))

	// atomically add new centroids to the SPTAG index and delete the old one
	err = l.SPTAG.Split(postingID, leftID, rightID, result.LeftCentroid, result.RightCentroid)
	if err != nil {
		return errors.Wrapf(err, "failed to split centroid for posting %d into %d and %d", postingID, leftID, rightID)
	}

	// TODO: enqueue reassign operations

	return nil
}

func (l *LocalRebuilder) doMerge(op MergeOperation) error {
	defer l.mergeList.done(op.PostingID) // Ensure we mark the operation as done

	l.postingLocks.Lock(op.PostingID)
	defer l.postingLocks.Unlock(op.PostingID)

	// Ensure the posting exists in the index
	if !l.SPTAG.Exists(op.PostingID) {
		l.Logger.WithField("postingID", op.PostingID).
			Debug("Posting not found, skipping merge operation")
		return nil // Nothing to merge
	}

	p, err := l.Store.Get(l.ctx, op.PostingID)
	if err != nil {
		if err == ErrPostingNotFound {
			l.Logger.WithField("postingID", op.PostingID).
				Debug("Posting not found, skipping merge operation")
			return nil
		}

		return errors.Wrapf(err, "failed to get posting %d for merge operation", op.PostingID)
	}

	// garbage collect the deleted vectors
	vectorSet := make(map[uint64]struct{})
	merged := make(Posting, 0, len(p))
	for _, v := range p {
		version := l.VersionMap.Get(v.ID)
		if version.Deleted() || version.Version() > v.Version.Version() {
			continue
		}
		merged = append(merged, Vector{
			ID:      v.ID,
			Version: version,
			Data:    v.Data,
		})
		vectorSet[v.ID] = struct{}{}
	}
	prevLen := len(merged)

	// skip if the posting is big enough
	if len(merged) >= l.UserConfig.MinPostingSize {
		l.Logger.
			WithField("postingID", op.PostingID).
			WithField("size", len(merged)).
			WithField("min", l.UserConfig.MinPostingSize).
			Debug("Posting is big enough, skipping merge operation")

		if len(merged) == len(p) {
			// no changes, just return
			return nil
		}

		// update the size of the posting
		l.PostingSizes.Set(op.PostingID, uint32(len(merged)))

		// persist the gc'ed posting
		err = l.Store.Put(l.ctx, op.PostingID, merged)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d", op.PostingID)
		}

		return nil
	}

	// get posting centroid
	oldCentroid := l.SPTAG.Get(op.PostingID)
	if oldCentroid == nil {
		return errors.Errorf("centroid not found for posting %d", op.PostingID)
	}

	// search for the closest centroids
	nearest, err := l.SPTAG.Search(oldCentroid, l.UserConfig.MergePostingCandidates)
	if err != nil {
		return errors.Wrapf(err, "failed to search for nearest centroid for posting %d", op.PostingID)
	}

	if len(nearest) <= 1 {
		l.Logger.WithField("postingID", op.PostingID).
			Debug("No candidates found for merge operation, skipping")

		// update the size of the posting
		l.PostingSizes.Set(op.PostingID, uint32(len(merged)))

		// persist the gc'ed posting
		err = l.Store.Put(l.ctx, op.PostingID, merged)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d", op.PostingID)
		}

		return nil
	}

	// first centroid is the query centroid, the rest are candidates for merging
	for i := 1; i < len(nearest); i++ {
		// check if the combined size of the postings is within limits
		count := l.PostingSizes.Get(nearest[i])
		if int(count)+len(merged) > l.UserConfig.MaxPostingSize {
			continue // Skip this candidate
		}

		// lock the candidate posting to ensure no concurrent modifications
		// note: the candidate lock might be the same as the current posting lock
		// so we need to ensure we don't deadlock
		if l.postingLocks.Hash(op.PostingID) != l.postingLocks.Hash(nearest[i]) {
			// lock the candidate posting
			l.postingLocks.Lock(nearest[i])
			defer l.postingLocks.Unlock(nearest[i])
		}

		// get the candidate posting
		candidate, err := l.Store.Get(l.ctx, nearest[i])
		if err != nil {
			return errors.Wrapf(err, "failed to get candidate posting %d for merge operation on posting %d", nearest[i], op.PostingID)
		}

		var candidateLen int
		for _, v := range candidate {
			version := l.VersionMap.Get(v.ID)
			if version.Deleted() || version.Version() > v.Version.Version() {
				continue
			}
			if _, exists := vectorSet[v.ID]; exists {
				continue // Skip duplicate vectors
			}
			merged = append(merged, Vector{
				ID:      v.ID,
				Version: version,
				Data:    v.Data,
			})
			candidateLen++
		}

		if prevLen > candidateLen {
			err = l.SPTAG.Delete(nearest[i])
			if err != nil {
				return errors.Wrapf(err, "failed to delete centroid %d for posting %d", nearest[i], op.PostingID)
			}

			// set the candidate posting size to 0 and update the current posting size
			l.PostingSizes.Set(nearest[i], 0)
			l.PostingSizes.Set(op.PostingID, uint32(len(merged)))

			err = l.Store.Put(l.ctx, op.PostingID, merged)
			if err != nil {
				return errors.Wrapf(err, "failed to put merged posting %d after merge operation", op.PostingID)
			}
		} else {
			err = l.SPTAG.Delete(op.PostingID)
			if err != nil {
				return errors.Wrapf(err, "failed to delete old centroid %d for posting %d", op.PostingID, nearest[i])
			}

			// set the current posting size to 0 and update the candidate posting size
			l.PostingSizes.Set(op.PostingID, 0)
			l.PostingSizes.Set(nearest[i], uint32(len(merged)))

			err = l.Store.Put(l.ctx, nearest[i], merged)
			if err != nil {
				return errors.Wrapf(err, "failed to put merged posting %d after merge operation", nearest[i])
			}
		}

		// TODO: enqueue a reassign operation to ensure the new posting is balanced

		return nil // Successfully merged and persisted
	}

	return nil
}

// deduplicator ensures only one operation per posting ID can be enqueued at a time.
type deduplicator struct {
	inflight *xsync.Map[uint64, struct{}] // map of inflight operations by posting ID
}

func newDeduplicator() *deduplicator {
	return &deduplicator{
		inflight: xsync.NewMap[uint64, struct{}](),
	}
}

func (d *deduplicator) tryEnqueue(id uint64) bool {
	_, exists := d.inflight.LoadAndStore(id, struct{}{})
	return !exists
}

func (d *deduplicator) done(id uint64) {
	d.inflight.Delete(id)
}
