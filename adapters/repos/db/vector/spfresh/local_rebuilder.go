//                           _       _
// __      _____  __ ___   ___  __ _| |_ ___
// \ \ /\ / / _ \/ _` \ \ / / |/ _` | __/ _ \
//  \ V  V /  __/ (_| |\ V /| | (_| | ||  __/
//   \_/\_/ \___|\__,_| \_/ |_|\__,_|\__\___|
//
//  Copyright Â© 2016 - 2025 Weaviate B.V. All rights reserved.
//
//  CONTACT: hello@weaviate.io
//

package spfresh

import (
	"context"
	"sync"

	"github.com/pkg/errors"
	"github.com/puzpuzpuz/xsync/v4"
	"github.com/sirupsen/logrus"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/common"
	enterrors "github.com/weaviate/weaviate/entities/errors"
)

type BackgroundOpType string

const (
	BackgroundOpSplit    BackgroundOpType = "split"
	BackgroundOpReassign BackgroundOpType = "reassign"
)

type Operation struct {
	OpType    BackgroundOpType
	PostingID uint64
	LeftID    uint64 // Used for reassign operations
	RightID   uint64 // Used for reassign operations
}

type MergeOperation struct {
	PostingID uint64
}

// LocalRebuilder manages background operations for postings in the SPFresh index.
// It handles split, merge, and reassign requests generated by insertions or during search
// operations. It uses a worker pool to process these operations concurrently.
type LocalRebuilder struct {
	UserConfig *UserConfig      // UserConfig contains user-defined settings for the rebuilder.
	SPTAG      SPTAG            // SPTAG provides access to the SPTAG index for centroid operations.
	Store      *BlockController // Used for managing persistence of postings.
	Logger     *logrus.Entry    // Logger for logging operations and errors.
	Splitter   PostingSplitter  // Used for splitting postings into two.
	VersionMap *VersionMap      // VersionMap provides access to vector versions.
	IDs        *common.MonotonicCounter[uint64]

	ctx    context.Context
	cancel context.CancelFunc

	ch      chan Operation
	mergeCh *xsync.UMPSCQueue[MergeOperation] // Unbounded channel for merge operations
	wg      sync.WaitGroup

	dedup *deduplicator // Deduplicator to prevent multiple operations on the same posting
}

func (l *LocalRebuilder) Start(ctx context.Context) {

	if l.UserConfig == nil {
		panic("UserConfig must be set before starting LocalRebuilder")
	}
	if l.Store == nil {
		panic("Store must be set before starting LocalRebuilder")
	}
	if l.Splitter == nil {
		panic("Splitter must be set before starting LocalRebuilder")
	}
	if l.VersionMap == nil {
		panic("VersionMap must be set before starting LocalRebuilder")
	}
	if l.IDs == nil {
		panic("IdGenerator must be set before starting LocalRebuilder")
	}

	l.ctx, l.cancel = context.WithCancel(context.Background())

	if l.Logger == nil {
		l.Logger = logrus.New().WithField("component", "LocalRebuilder")
	} else {
		l.Logger = l.Logger.WithField("component", "LocalRebuilder")
	}

	l.ch = make(chan Operation, l.UserConfig.Workers)
	l.mergeCh = xsync.NewUMPSCQueue[MergeOperation]()

	// start N workers to process split and reassign operations
	for i := 0; i < l.UserConfig.Workers; i++ {
		l.wg.Add(1)
		enterrors.GoWrapper(l.worker, l.Logger)
	}

	// start a single worker to process merge operations
	l.wg.Add(1)
	enterrors.GoWrapper(l.mergeWorker, l.Logger)
}

func (l *LocalRebuilder) Enqueue(ctx context.Context, op Operation) error {
	if l.ctx == nil {
		return nil // Not started yet
	}

	if l.ctx.Err() != nil {
		return l.ctx.Err() // Context already cancelled
	}

	// Check if the operation is already in progress
	if !l.dedup.tryEnqueue(op) {
		l.Logger.WithField("postingID", op.PostingID).
			WithField("operation", op.OpType).
			Debug("Operation already in progress, skipping enqueue")
		return nil
	}

	// Enqueue the operation to the channel
	select {
	case l.ch <- op:
	case <-ctx.Done():
		return ctx.Err()
	}

	return nil
}

func (l *LocalRebuilder) enqueueOrSteal(op Operation) error {
	select {
	case l.ch <- op:
		return nil // Successfully enqueued
	default:
	}

	// Channel is full, process the operation immediately
	return l.process(op)
}

func (l *LocalRebuilder) worker() {
	defer l.wg.Done()

	for op := range l.ch {
		if l.ctx.Err() != nil {
			return
		}

		err := l.process(op)
		if err != nil {
			l.Logger.WithError(err).
				WithField("postingID", op.PostingID).
				WithField("operation", op.OpType).
				Error("Failed to process operation")
			continue // Log the error and continue processing other operations
		}
		l.dedup.done(op) // Ensure we mark the operation as done
	}
}

func (l *LocalRebuilder) mergeWorker() {
	defer l.wg.Done()

	for {
		op := l.mergeCh.Dequeue()

		if l.ctx.Err() != nil {
			return // Exit if the context is cancelled
		}

		// A zero-value MergeOperation indicates no more operations to process
		if op.PostingID == 0 {
			// No more merge operations to process
			return
		}

		err := l.doMerge(op)
		if err != nil {
			l.Logger.WithError(err).
				WithField("postingID", op.PostingID).
				Error("Failed to process merge operation")
			continue // Log the error and continue processing other operations
		}
	}
}

func (l *LocalRebuilder) process(op Operation) error {
	// Process the operation
	switch op.OpType {
	case BackgroundOpSplit:
		// Handle split operation
		return l.doSplit(op)
	case BackgroundOpReassign:
		// Handle reassign operation
	default:
		l.Logger.Warnf("Unknown operation type: %s for posting ID: %d", op.OpType, op.PostingID)
	}

	return nil
}

func (l *LocalRebuilder) Close(ctx context.Context) error {
	if l.ctx == nil {
		return nil // Already closed or not started
	}

	if l.ctx.Err() != nil {
		return l.ctx.Err() // Context already cancelled
	}

	// Cancel the context to prevent new operations from being enqueued
	l.cancel()

	// Close the channel to signal workers to stop
	close(l.ch)

	// Close the merge channel by enqueuing a zero-value MergeOperation
	l.mergeCh.Enqueue(MergeOperation{})

	l.wg.Wait() // Wait for all workers to finish
	return nil
}

func (l *LocalRebuilder) doSplit(op Operation) error {
	p, err := l.Store.Get(l.ctx, op.PostingID)
	if err != nil {
		if err == ErrPostingNotFound {
			l.Logger.WithField("postingID", op.PostingID).
				Debug("Posting not found, skipping split operation")
			return nil
		}

		return errors.Wrapf(err, "failed to get posting %d for split operation", op.PostingID)
	}

	// garbage collect the deleted vectors
	filtered := p.GarbageCollect(l.VersionMap)

	// skip if the filtered posting is now too small
	if len(filtered) < l.UserConfig.MaxPostingSize {
		l.Logger.
			WithField("postingID", op.PostingID).
			WithField("size", len(filtered)).
			WithField("max", l.UserConfig.MaxPostingSize).
			Debug("Posting has less than max size after garbage collection, skipping split operation")

		if len(filtered) == len(p) {
			// no changes, just return
			return nil
		}

		// persist the gc'ed posting
		err = l.Store.Put(l.ctx, op.PostingID, filtered)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d after split operation", op.PostingID)
		}

		return nil
	}

	// split the vectors into two clusters
	result, err := l.Splitter.Split(filtered)
	if err != nil {
		return errors.Wrapf(err, "failed to split vectors for posting %d", op.PostingID)
	}

	// persist the new postings first
	leftID, rightID := l.IDs.NextN(2)
	err = l.Store.Put(l.ctx, leftID, result.LeftPosting)
	if err != nil {
		return errors.Wrapf(err, "failed to put left posting %d for split operation on posting %d", leftID, op.PostingID)
	}
	err = l.Store.Put(l.ctx, rightID, result.RightPosting)
	if err != nil {
		// cleanup will be handled by the snapshot process
		return errors.Wrapf(err, "failed to put right posting %d for split operation on posting %d", rightID, op.PostingID)
	}

	// atomically add new centroids to the SPTAG index and delete the old one
	err = l.SPTAG.Split(op.PostingID, leftID, rightID, result.LeftCentroid, result.RightCentroid)
	if err != nil {
		return errors.Wrapf(err, "failed to split centroid for posting %d into %d and %d", op.PostingID, leftID, rightID)
	}

	// enqueue a reassign operation to ensure the new postings are balanced
	err = l.enqueueOrSteal(Operation{
		OpType:    BackgroundOpReassign,
		PostingID: leftID,
		RightID:   rightID,
	})
	if err != nil {
		return errors.Wrapf(err, "failed to enqueue or steal reassign operation for posting %d", op.PostingID)
	}

	return nil
}

func (l *LocalRebuilder) doMerge(op MergeOperation) error {
	p, err := l.Store.Get(l.ctx, op.PostingID)
	if err != nil {
		if err == ErrPostingNotFound {
			l.Logger.WithField("postingID", op.PostingID).
				Debug("Posting not found, skipping merge operation")
			return nil
		}

		return errors.Wrapf(err, "failed to get posting %d for merge operation", op.PostingID)
	}

	// garbage collect the deleted vectors
	vectorSet := make(map[uint64]struct{})
	var merged Posting
	for _, v := range p {
		version := l.VersionMap.Get(v.ID)
		if version.Deleted() || version.Version() > v.Version.Version() {
			continue
		}
		merged = append(merged, Vector{
			ID:      v.ID,
			Version: version,
			Data:    v.Data,
		})
		vectorSet[v.ID] = struct{}{}
	}
	prevLen := len(merged)

	// skip if the posting is big enough
	if len(merged) >= l.UserConfig.MinPostingSize {
		l.Logger.
			WithField("postingID", op.PostingID).
			WithField("size", len(merged)).
			WithField("min", l.UserConfig.MinPostingSize).
			Debug("Posting is big enough, skipping merge operation")

		if len(merged) == len(p) {
			// no changes, just return
			return nil
		}

		// persist the gc'ed posting
		err = l.Store.Put(l.ctx, op.PostingID, merged)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d", op.PostingID)
		}

		return nil
	}

	// get posting centroid
	oldCentroid := l.SPTAG.Get(op.PostingID)
	if oldCentroid == nil {
		return errors.Errorf("centroid not found for posting %d", op.PostingID)
	}

	// search for the closest centroids
	nearest, err := l.SPTAG.Search(oldCentroid, l.UserConfig.MergePostingCandidates)
	if err != nil {
		return errors.Wrapf(err, "failed to search for nearest centroid for posting %d", op.PostingID)
	}

	if len(nearest) <= 1 {
		l.Logger.WithField("postingID", op.PostingID).
			Debug("No candidates found for merge operation, skipping")

		// persist the gc'ed posting
		err = l.Store.Put(l.ctx, op.PostingID, merged)
		if err != nil {
			return errors.Wrapf(err, "failed to put filtered posting %d", op.PostingID)
		}

		return nil
	}

	// first centroid is the query centroid, the rest are candidates for merging
	for i := 1; i < len(nearest); i++ {
		// check if the combined size of the postings is within limits
		count, err := l.Store.VectorCount(nearest[i])
		if err != nil {
			return errors.Wrapf(err, "failed to get vector count for posting %d", nearest[i])
		}
		if count+len(merged) > l.UserConfig.MaxPostingSize {
			continue // Skip this candidate
		}

		// TODO: ensure posting is not already being merged/split

		// get the candidate posting
		candidate, err := l.Store.Get(l.ctx, nearest[i])
		if err != nil {
			return errors.Wrapf(err, "failed to get candidate posting %d for merge operation on posting %d", nearest[i], op.PostingID)
		}

		var candidateLen int
		for _, v := range candidate {
			version := l.VersionMap.Get(v.ID)
			if version.Deleted() || version.Version() > v.Version.Version() {
				continue
			}
			if _, exists := vectorSet[v.ID]; exists {
				continue // Skip duplicate vectors
			}
			merged = append(merged, Vector{
				ID:      v.ID,
				Version: version,
				Data:    v.Data,
			})
			candidateLen++
		}

		if prevLen > candidateLen {
			err = l.SPTAG.Delete(nearest[i])
			if err != nil {
				return errors.Wrapf(err, "failed to delete centroid %d for posting %d", nearest[i], op.PostingID)
			}
			err = l.Store.Put(l.ctx, op.PostingID, merged)
			if err != nil {
				return errors.Wrapf(err, "failed to put merged posting %d after merge operation", op.PostingID)
			}
			// TODO: delete the old posting
		} else {
			err = l.SPTAG.Delete(op.PostingID)
			if err != nil {
				return errors.Wrapf(err, "failed to delete old centroid %d for posting %d", op.PostingID, nearest[i])
			}
			err = l.Store.Put(l.ctx, nearest[i], merged)
			if err != nil {
				return errors.Wrapf(err, "failed to put merged posting %d after merge operation", nearest[i])
			}
			// TODO: delete the old posting
		}

		// TODO: enqueue a reassign operation to ensure the new posting is balanced

		return nil // Successfully merged and persisted
	}

	return nil
}

// deduplicator ensures only one operation per posting ID can be enqueued at a time.
type deduplicator struct {
	mu       sync.Mutex
	inflight map[Operation]struct{} // map of inflight operations by posting ID
}

func newDeduplicator() *deduplicator {
	return &deduplicator{
		inflight: make(map[Operation]struct{}),
	}
}

func (d *deduplicator) tryEnqueue(op Operation) bool {
	d.mu.Lock()
	defer d.mu.Unlock()

	if _, exists := d.inflight[op]; exists {
		return false // Operation already in progress
	}

	d.inflight[op] = struct{}{}
	return true
}

func (d *deduplicator) done(op Operation) {
	d.mu.Lock()
	defer d.mu.Unlock()

	delete(d.inflight, op)
}
